{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSnGm9zU6ISa"
      },
      "source": [
        "# Vnemo Transaction Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3yE8ar3zYkC",
        "outputId": "11b39f20-0497-4a27-c40e-2b8dcb6902fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "W55J4wAr5X0D",
        "outputId": "0d24e48f-1ff6-4e9d-aa35-88ae8e9b3182"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Collecting graphframes\n",
            "  Downloading graphframes-0.6-py2.py3-none-any.whl.metadata (934 bytes)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from graphframes) (2.0.2)\n",
            "Collecting nose (from graphframes)\n",
            "  Downloading nose-1.3.7-py3-none-any.whl.metadata (1.7 kB)\n",
            "Downloading graphframes-0.6-py2.py3-none-any.whl (18 kB)\n",
            "Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nose, graphframes\n",
            "Successfully installed graphframes-0.6 nose-1.3.7\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m111.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m113.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m127.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, torch-geometric, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torch-geometric-2.6.1\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.1\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies (for future questions)\n",
        "!pip install pyspark\n",
        "!pip install graphframes\n",
        "!pip install networkx\n",
        "!pip install matplotlib seaborn\n",
        "!pip install torch torch-geometric\n",
        "!pip install transformers\n",
        "!pip install emoji\n",
        "\n",
        "# Import libraries (for future questions)\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lower, trim, udf, explode\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from graphframes import GraphFrame\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from pyspark.sql.types import BooleanType, ArrayType, StringType\n",
        "import pandas as pd\n",
        "import emoji\n",
        "import re\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgU_1Cvc5gMR"
      },
      "source": [
        "## 1. Text Analytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfINhKKk6MUB"
      },
      "source": [
        "### 1.1 Use the text dictionary and the emoji dictionary to classify Venmoâ€™s transactions in your sample dataset. What is the percent of emoji only transactions? Which are the top 5 most popular emoji? Which are the top three most popular emoji categories?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kEn5UXt6rvd",
        "outputId": "7dc0f0c9-3f96-48a1-ddc9-24aa176d2f10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Venmo dataset loaded successfully.\n",
            "Descriptions cleaned successfully.\n",
            "Word dictionary loaded successfully.\n",
            "Emoji dictionary loaded successfully.\n",
            "Emoji-only transactions flagged.\n",
            "\n",
            "Top 5 Uncategorized Emojis (None category):\n",
            "Emoji: ğŸ’¸, Count: 124727\n",
            "Emoji: ğŸ , Count: 65987\n",
            "Emoji: â¤ï¸, Count: 56701\n",
            "Emoji: ğŸ¡, Count: 30932\n",
            "Emoji: ğŸ’°, Count: 28303\n",
            "Percent of Emoji-Only Transactions: 32.35%\n",
            "\n",
            "Top 5 Emojis:\n",
            "Emoji: PizzaSlice, Count: 215039\n",
            "Emoji: BeerMugs, Count: 145233\n",
            "Emoji: MoneyWithWings, Count: 124727\n",
            "Emoji: WineGlass, Count: 111157\n",
            "Emoji: PartyPopper, Count: 94327\n",
            "\n",
            "Top 3 Emoji Categories (excluding None):\n",
            "Category: Food, Count: 1744262\n",
            "Category: People, Count: 787257\n",
            "Category: Activity, Count: 405737\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col as spark_col, lower as spark_lower, trim as spark_trim, udf, explode\n",
        "from pyspark.sql.types import BooleanType, ArrayType, StringType\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import emoji\n",
        "import re\n",
        "\n",
        "# Reset Matplotlib font settings to default\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"VenmoAnalysisQ2_Full\") \\\n",
        "    .config(\"spark.driver.memory\", \"8g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# File paths\n",
        "venmo_path = \"/content/VenmoSample.snappy.parquet\"\n",
        "word_dict_path = \"/content/Venmo Word Classification Dictonary BAX-423.xlsx\"\n",
        "emoji_dict_path = \"/content/Venmo_Emoji_Classification_Dictionary.csv\"\n",
        "\n",
        "# Load Venmo dataset\n",
        "try:\n",
        "    df = spark.read.parquet(venmo_path)\n",
        "    print(\"Venmo dataset loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading Venmo dataset: {e}\")\n",
        "    spark.stop()\n",
        "    raise e\n",
        "\n",
        "# Clean descriptions\n",
        "try:\n",
        "    df = df.withColumn(\"description\", spark_lower(spark_trim(spark_col(\"description\")))) \\\n",
        "           .na.drop(subset=[\"description\"])\n",
        "    print(\"Descriptions cleaned successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error cleaning descriptions: {e}\")\n",
        "    spark.stop()\n",
        "    raise e\n",
        "\n",
        "# Load word dictionary\n",
        "try:\n",
        "    word_dict_pd = pd.read_excel(word_dict_path, sheet_name=\"Word_Dict\")\n",
        "    print(\"Word dictionary loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading word dictionary: {e}\")\n",
        "    spark.stop()\n",
        "    raise e\n",
        "\n",
        "# Convert word dictionary to Spark DataFrame\n",
        "word_dict_melted = []\n",
        "for category in word_dict_pd.columns[:-1]:\n",
        "    words = word_dict_pd[category].dropna().tolist()\n",
        "    for word in words:\n",
        "        word_dict_melted.append((word, category))\n",
        "word_dict_df = spark.createDataFrame(word_dict_melted, [\"word\", \"category\"])\n",
        "\n",
        "# Load emoji dictionary\n",
        "try:\n",
        "    emoji_dict_df = spark.read.csv(emoji_dict_path, header=True)\n",
        "    print(\"Emoji dictionary loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading emoji dictionary: {e}\")\n",
        "    spark.stop()\n",
        "    raise e\n",
        "\n",
        "# Melt emoji dictionary\n",
        "emoji_dict_melted = []\n",
        "for col in emoji_dict_df.columns:\n",
        "    emojis = emoji_dict_df.select(col).dropna().collect()\n",
        "    for row in emojis:\n",
        "        emoji_dict_melted.append((row[0], col))\n",
        "emoji_dict_spark = spark.createDataFrame(emoji_dict_melted, [\"emoji\", \"category\"])\n",
        "\n",
        "# Define UDF for emoji-only detection\n",
        "def is_emoji_only(text):\n",
        "    if not text:\n",
        "        return False\n",
        "    cleaned = re.sub(r'[a-zA-Z0-9\\s]', '', text)\n",
        "    return len(cleaned) > 0 and all(any(c in item['emoji'] for item in emoji.emoji_list(text)) for c in cleaned)\n",
        "is_emoji_only_udf = udf(is_emoji_only, BooleanType())\n",
        "\n",
        "# Flag emoji-only transactions\n",
        "try:\n",
        "    df = df.withColumn(\"is_emoji_only\", is_emoji_only_udf(spark_col(\"description\")))\n",
        "    print(\"Emoji-only transactions flagged.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error flagging emoji-only transactions: {e}\")\n",
        "    spark.stop()\n",
        "    raise e\n",
        "\n",
        "# Calculate percent emoji-only\n",
        "total_transactions = df.count()\n",
        "emoji_only_count = df.filter(spark_col(\"is_emoji_only\")).count()\n",
        "percent_emoji_only = (emoji_only_count / total_transactions) * 100 if total_transactions > 0 else 0\n",
        "\n",
        "# Define UDF for emoji extraction\n",
        "def extract_emojis(text):\n",
        "    return [item['emoji'] for item in emoji.emoji_list(text)]\n",
        "extract_emojis_udf = udf(extract_emojis, ArrayType(StringType()))\n",
        "\n",
        "# Extract emojis\n",
        "df_emojis = df.withColumn(\"emojis\", extract_emojis_udf(spark_col(\"description\")))\n",
        "df_emojis_exploded = df_emojis.select(explode(spark_col(\"emojis\")).alias(\"emoji\"))\n",
        "\n",
        "# Count emoji frequencies\n",
        "emoji_counts = df_emojis_exploded.groupBy(\"emoji\").count().orderBy(spark_col(\"count\").desc())\n",
        "top_5_emojis = emoji_counts.limit(5).collect()\n",
        "\n",
        "# Debug: Find uncategorized emojis\n",
        "uncategorized_emojis = df_emojis_exploded.join(emoji_dict_spark, \"emoji\", \"left\") \\\n",
        "    .filter(spark_col(\"category\").isNull()) \\\n",
        "    .groupBy(\"emoji\").count().orderBy(spark_col(\"count\").desc())\n",
        "top_5_uncategorized = uncategorized_emojis.limit(5).collect()\n",
        "print(\"\\nTop 5 Uncategorized Emojis (None category):\")\n",
        "for row in top_5_uncategorized:\n",
        "    print(f\"Emoji: {row['emoji']}, Count: {row['count']}\")\n",
        "\n",
        "# Map emojis to categories (exclude None)\n",
        "emoji_category_counts = df_emojis_exploded.join(emoji_dict_spark, \"emoji\", \"left\") \\\n",
        "    .groupBy(\"category\").count().orderBy(spark_col(\"count\").desc())\n",
        "top_3_categories = emoji_category_counts.filter(spark_col(\"category\").isNotNull()).limit(3).collect()\n",
        "\n",
        "# Convert results to pandas, using simplified text labels for all emojis\n",
        "emoji_names = {\n",
        "    \"ğŸ•\": \"PizzaSlice\",\n",
        "    \"ğŸ»\": \"BeerMugs\",\n",
        "    \"ğŸ’¸\": \"MoneyWithWings\",\n",
        "    \"ğŸ·\": \"WineGlass\",\n",
        "    \"ğŸ‰\": \"PartyPopper\",\n",
        "    \"ğŸˆ\": \"Balloon\",\n",
        "    \"â¤ï¸\": \"RedHeart\",\n",
        "    \"ğŸ¡\": \"HouseWithGarden\",\n",
        "    \"ğŸ \": \"House\",\n",
        "    \"ğŸ˜­\": \"CryingFace\",\n",
        "    \"ğŸ¹\": \"TropicalDrink\",\n",
        "    \"ğŸ’°\": \"MoneyBag\"\n",
        "}\n",
        "top_5_emojis_df = pd.DataFrame([(row[\"emoji\"], row[\"count\"]) for row in top_5_emojis], columns=[\"Emoji\", \"Count\"])\n",
        "top_5_emojis_df[\"Emoji\"] = top_5_emojis_df[\"Emoji\"].map(emoji_names).fillna(top_5_emojis_df[\"Emoji\"])\n",
        "top_3_categories_df = pd.DataFrame([(row[\"category\"], row[\"count\"]) for row in top_3_categories], columns=[\"Category\", \"Count\"])\n",
        "\n",
        "# Plot top 5 emojis with simplified text labels\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x=\"Count\", y=\"Emoji\", data=top_5_emojis_df)\n",
        "plt.title(\"Top 5 Most Popular Emojis in Venmo Transactions\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"/content/top_5_emojis.png\")\n",
        "plt.close()\n",
        "\n",
        "# Plot top 3 categories\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x=\"Count\", y=\"Category\", data=top_3_categories_df)\n",
        "plt.title(\"Top 3 Most Popular Emoji Categories in Venmo Transactions\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"/content/top_3_categories.png\")\n",
        "plt.close()\n",
        "\n",
        "# Print results\n",
        "print(f\"Percent of Emoji-Only Transactions: {percent_emoji_only:.2f}%\")\n",
        "print(\"\\nTop 5 Emojis:\")\n",
        "for row in top_5_emojis:\n",
        "    emoji_label = emoji_names.get(row['emoji'], row['emoji'])\n",
        "    print(f\"Emoji: {emoji_label}, Count: {row['count']}\")\n",
        "print(\"\\nTop 3 Emoji Categories (excluding None):\")\n",
        "for row in top_3_categories:\n",
        "    print(f\"Category: {row['category']}, Count: {row['count']}\")\n",
        "\n",
        "# Save cleaned DataFrame for future questions\n",
        "df.write.mode(\"overwrite\").parquet(\"/content/venmo_cleaned.parquet\")\n",
        "\n",
        "# Stop SparkSession\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i0JeLa5Zh6T"
      },
      "source": [
        "### 1.2 For each user, create variables to classify their spending behavior profile into categories. For example, if a user has made 10 transactions, where 5 of them are food and the other 5 are activity, then the userâ€™s spending profile will be 50% food and 50% activity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvDxreKtL43c",
        "outputId": "3acca9d1-6420-48cc-d4b0-d1ead4b298a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Venmo dataset loaded successfully.\n",
            "Word dictionary loaded successfully.\n",
            "Emoji dictionary loaded successfully.\n",
            "Sample of User Spending Profiles (first 10 rows):\n",
            "+-----+---------------+----------+\n",
            "|user1|category       |percentage|\n",
            "+-----+---------------+----------+\n",
            "|3    |Food           |33.333332 |\n",
            "|3    |People         |33.333332 |\n",
            "|3    |Utility        |33.333332 |\n",
            "|4    |Activity       |33.333332 |\n",
            "|4    |Food           |33.333332 |\n",
            "|4    |Illegal/Sarcasm|16.666666 |\n",
            "|4    |Travel         |16.666666 |\n",
            "|10   |Activity       |10.0      |\n",
            "|10   |Food           |60.0      |\n",
            "|10   |People         |20.0      |\n",
            "+-----+---------------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col as spark_col, lower as spark_lower, trim as spark_trim, udf, explode, split, lit, sum as spark_sum\n",
        "from pyspark.sql.types import ArrayType, StringType, StructType, StructField\n",
        "import pandas as pd\n",
        "import emoji\n",
        "import re\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"VenmoAnalysisQ3\") \\\n",
        "    .config(\"spark.driver.memory\", \"8g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# File paths\n",
        "venmo_path = \"/content/venmo_cleaned.parquet\"\n",
        "word_dict_path = \"/content/Venmo Word Classification Dictonary BAX-423.xlsx\"\n",
        "emoji_dict_path = \"/content/Venmo_Emoji_Classification_Dictionary.csv\"\n",
        "\n",
        "# Load cleaned Venmo dataset from Q2\n",
        "try:\n",
        "    df = spark.read.parquet(venmo_path)\n",
        "    print(\"Venmo dataset loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading Venmo dataset: {e}\")\n",
        "    spark.stop()\n",
        "    raise e\n",
        "\n",
        "# Load word dictionary\n",
        "try:\n",
        "    word_dict_pd = pd.read_excel(word_dict_path, sheet_name=\"Word_Dict\")\n",
        "    print(\"Word dictionary loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading word dictionary: {e}\")\n",
        "    spark.stop()\n",
        "    raise e\n",
        "\n",
        "# Convert word dictionary to Spark DataFrame\n",
        "word_dict_melted = []\n",
        "for category in word_dict_pd.columns[:-1]:  # Exclude @dropdown\n",
        "    words = word_dict_pd[category].dropna().tolist()\n",
        "    for word in words:\n",
        "        word_dict_melted.append((word, category))\n",
        "word_dict_df = spark.createDataFrame(word_dict_melted, [\"word\", \"category\"]).withColumn(\"word\", spark_lower(spark_col(\"word\")))\n",
        "\n",
        "# Load emoji dictionary\n",
        "try:\n",
        "    emoji_dict_df = spark.read.csv(emoji_dict_path, header=True)\n",
        "    print(\"Emoji dictionary loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading emoji dictionary: {e}\")\n",
        "    spark.stop()\n",
        "    raise e\n",
        "\n",
        "# Melt emoji dictionary\n",
        "emoji_dict_melted = []\n",
        "for col in emoji_dict_df.columns:\n",
        "    emojis = emoji_dict_df.select(col).dropna().collect()\n",
        "    for row in emojis:\n",
        "        emoji_dict_melted.append((row[0], col))\n",
        "emoji_dict_spark = spark.createDataFrame(emoji_dict_melted, [\"emoji\", \"category\"])\n",
        "\n",
        "# Define UDF to extract words from description\n",
        "def extract_words(text):\n",
        "    if not text:\n",
        "        return []\n",
        "    # Split on whitespace, remove punctuation\n",
        "    words = re.split(r'\\s+', text.strip())\n",
        "    return [word.lower() for word in words if word]\n",
        "extract_words_udf = udf(extract_words, ArrayType(StringType()))\n",
        "\n",
        "# Define UDF for emoji extraction\n",
        "def extract_emojis(text):\n",
        "    return [item['emoji'] for item in emoji.emoji_list(text)]\n",
        "extract_emojis_udf = udf(extract_emojis, ArrayType(StringType()))\n",
        "\n",
        "# Extract words and emojis from descriptions\n",
        "df = df.withColumn(\"words\", extract_words_udf(spark_col(\"description\")))\n",
        "df = df.withColumn(\"emojis\", extract_emojis_udf(spark_col(\"description\")))\n",
        "\n",
        "# Explode words and join with word dictionary\n",
        "df_words = df.select(\"user1\", explode(spark_col(\"words\")).alias(\"word\")) \\\n",
        "             .join(word_dict_df, \"word\", \"left\") \\\n",
        "             .filter(spark_col(\"category\").isNotNull()) \\\n",
        "             .select(\"user1\", \"category\")\n",
        "\n",
        "# Explode emojis and join with emoji dictionary\n",
        "df_emojis = df.select(\"user1\", explode(spark_col(\"emojis\")).alias(\"emoji\")) \\\n",
        "              .join(emoji_dict_spark, \"emoji\", \"left\") \\\n",
        "              .filter(spark_col(\"category\").isNotNull()) \\\n",
        "              .select(\"user1\", \"category\")\n",
        "\n",
        "# Combine word and emoji categories\n",
        "df_categories = df_words.union(df_emojis)\n",
        "\n",
        "# Count category occurrences per user\n",
        "category_counts = df_categories.groupBy(\"user1\", \"category\").count()\n",
        "\n",
        "# Calculate total category occurrences per user\n",
        "total_counts = category_counts.groupBy(\"user1\").agg(spark_sum(\"count\").alias(\"total_count\"))\n",
        "\n",
        "# Join and calculate percentages\n",
        "user_profiles = category_counts.join(total_counts, \"user1\") \\\n",
        "                              .withColumn(\"percentage\", (spark_col(\"count\") / spark_col(\"total_count\") * 100).cast(\"float\")) \\\n",
        "                              .select(\"user1\", \"category\", \"percentage\") \\\n",
        "                              .orderBy(\"user1\", \"category\")\n",
        "\n",
        "# Save results\n",
        "user_profiles.write.mode(\"overwrite\").parquet(\"/content/user_spending_profiles.parquet\")\n",
        "\n",
        "# Print sample results\n",
        "print(\"Sample of User Spending Profiles (first 10 rows):\")\n",
        "user_profiles.show(10, truncate=False)\n",
        "\n",
        "# Stop SparkSession\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_1l_V2idLed"
      },
      "source": [
        "### 1.3 In the previous question, you got a static spending profile. However, life and social networks are evolving over time. Therefore, letâ€™s explore how a userâ€™s spending profile is evolving over her lifetime in Venmo. First of all, you need to analyze a userâ€™s transactions in monthly intervals, starting from 0 (indicating their first transaction only) up to 12."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVCqGLqsaaf0",
        "outputId": "aac93a1b-6e33-4ebd-a467-63bd7c3ee207"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (2.14.1)\n",
            "Venmo dataset loaded successfully.\n",
            "Word dictionary loaded successfully.\n",
            "Emoji dictionary loaded successfully.\n",
            "Sample Statistics (first 10 rows):\n",
            "+------------+---------------+-------------------+---------+\n",
            "|month_offset|category       |mean_percentage    |stddev   |\n",
            "+------------+---------------+-------------------+---------+\n",
            "|12          |Food           |2.010389650637637  |13.377702|\n",
            "|6           |Activity       |1.1555487713536534 |9.841908 |\n",
            "|0           |People         |12.760329067396402 |30.773056|\n",
            "|5           |Transportation |0.6818464025859586 |7.6585336|\n",
            "|9           |Illegal/Sarcasm|0.39361779175503914|5.7560086|\n",
            "|3           |Travel         |0.28679731027423416|4.8826413|\n",
            "|0           |Utility        |10.197559496424695 |28.69671 |\n",
            "|8           |People         |1.1243461408284632 |9.82256  |\n",
            "|12          |Activity       |0.8003213456688366 |8.181095 |\n",
            "|9           |Transportation |0.5702619742552362 |6.984153 |\n",
            "+------------+---------------+-------------------+---------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Ensure emoji library is installed\n",
        "!pip install emoji\n",
        "\n",
        "# Import libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col as spark_col, lower as spark_lower, trim as spark_trim, udf, explode, split, lit, sum as spark_sum, min as spark_min, floor, months_between, datediff, when\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "from pyspark.sql.window import Window\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')  # Use non-interactive backend\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import emoji\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"VenmoAnalysisQ4\") \\\n",
        "    .config(\"spark.driver.memory\", \"8g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# File paths\n",
        "venmo_path = \"/content/venmo_cleaned.parquet\"\n",
        "word_dict_path = \"/content/Venmo Word Classification Dictonary BAX-423.xlsx\"\n",
        "emoji_dict_path = \"/content/Venmo_Emoji_Classification_Dictionary.csv\"\n",
        "\n",
        "# Load cleaned Venmo dataset\n",
        "try:\n",
        "    df = spark.read.parquet(venmo_path)\n",
        "    print(\"Venmo dataset loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading Venmo dataset: {e}\")\n",
        "    spark.stop()\n",
        "    raise e\n",
        "\n",
        "# Load word dictionary\n",
        "try:\n",
        "    word_dict_pd = pd.read_excel(word_dict_path, sheet_name=\"Word_Dict\")\n",
        "    print(\"Word dictionary loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading word dictionary: {e}\")\n",
        "    spark.stop()\n",
        "    raise e\n",
        "\n",
        "# Convert word dictionary to Spark DataFrame\n",
        "word_dict_melted = []\n",
        "for category in word_dict_pd.columns[:-1]:\n",
        "    words = word_dict_pd[category].dropna().tolist()\n",
        "    for word in words:\n",
        "        word_dict_melted.append((word, category))\n",
        "word_dict_df = spark.createDataFrame(word_dict_melted, [\"word\", \"category\"]).withColumn(\"word\", spark_lower(spark_col(\"word\")))\n",
        "\n",
        "# Load emoji dictionary\n",
        "try:\n",
        "    emoji_dict_df = spark.read.csv(emoji_dict_path, header=True)\n",
        "    print(\"Emoji dictionary loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading emoji dictionary: {e}\")\n",
        "    spark.stop()\n",
        "    raise e\n",
        "\n",
        "# Melt emoji dictionary\n",
        "emoji_dict_melted = []\n",
        "for col in emoji_dict_df.columns:\n",
        "    emojis = emoji_dict_df.select(col).dropna().collect()\n",
        "    for row in emojis:\n",
        "        emoji_dict_melted.append((row[0], col))\n",
        "emoji_dict_spark = spark.createDataFrame(emoji_dict_melted, [\"emoji\", \"category\"])\n",
        "\n",
        "# Define UDF to extract words\n",
        "def extract_words(text):\n",
        "    if not text:\n",
        "        return []\n",
        "    words = re.split(r'\\s+', text.strip())\n",
        "    return [word.lower() for word in words if word]\n",
        "extract_words_udf = udf(extract_words, ArrayType(StringType()))\n",
        "\n",
        "# Define UDF for emoji extraction\n",
        "def extract_emojis(text):\n",
        "    return [item['emoji'] for item in emoji.emoji_list(text)]\n",
        "extract_emojis_udf = udf(extract_emojis, ArrayType(StringType()))\n",
        "\n",
        "# Extract words and emojis\n",
        "df = df.withColumn(\"words\", extract_words_udf(spark_col(\"description\")))\n",
        "df = df.withColumn(\"emojis\", extract_emojis_udf(spark_col(\"description\")))\n",
        "\n",
        "# Find first transaction date per user\n",
        "window_spec = Window.partitionBy(\"user1\").orderBy(\"datetime\")\n",
        "df = df.withColumn(\"first_transaction_date\", spark_min(\"datetime\").over(window_spec))\n",
        "\n",
        "# Calculate month offset from first transaction\n",
        "df = df.withColumn(\"month_offset\", floor(months_between(spark_col(\"datetime\"), spark_col(\"first_transaction_date\")))) \\\n",
        "       .filter(spark_col(\"month_offset\").between(0, 12))\n",
        "\n",
        "# Explode words and join with word dictionary\n",
        "df_words = df.select(\"user1\", \"month_offset\", explode(spark_col(\"words\")).alias(\"word\")) \\\n",
        "             .join(word_dict_df, \"word\", \"left\") \\\n",
        "             .filter(spark_col(\"category\").isNotNull()) \\\n",
        "             .select(\"user1\", \"month_offset\", \"category\")\n",
        "\n",
        "# Explode emojis and join with emoji dictionary\n",
        "df_emojis = df.select(\"user1\", \"month_offset\", explode(spark_col(\"emojis\")).alias(\"emoji\")) \\\n",
        "              .join(emoji_dict_spark, \"emoji\", \"left\") \\\n",
        "              .filter(spark_col(\"category\").isNotNull()) \\\n",
        "              .select(\"user1\", \"month_offset\", \"category\")\n",
        "\n",
        "# Combine word and emoji categories\n",
        "df_categories = df_words.union(df_emojis)\n",
        "\n",
        "# Create cumulative window up to each month\n",
        "cumulative_window = Window.partitionBy(\"user1\").orderBy(\"month_offset\") \\\n",
        "                         .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "\n",
        "# Count category occurrences per user and month (cumulative)\n",
        "category_counts = df_categories.groupBy(\"user1\", \"month_offset\", \"category\") \\\n",
        "                              .count() \\\n",
        "                              .withColumn(\"cumulative_count\", spark_sum(\"count\").over(cumulative_window))\n",
        "\n",
        "# Calculate total cumulative category occurrences per user and month\n",
        "total_counts = category_counts.groupBy(\"user1\", \"month_offset\") \\\n",
        "                             .agg(spark_sum(\"cumulative_count\").alias(\"total_count\"))\n",
        "\n",
        "# Join and calculate cumulative percentages\n",
        "user_profiles = category_counts.join(total_counts, [\"user1\", \"month_offset\"]) \\\n",
        "                              .withColumn(\"percentage\", (spark_col(\"cumulative_count\") / spark_col(\"total_count\") * 100).cast(\"float\")) \\\n",
        "                              .select(\"user1\", \"month_offset\", \"category\", \"percentage\")\n",
        "\n",
        "# Get all unique users and months\n",
        "users = user_profiles.select(\"user1\").distinct()\n",
        "months = user_profiles.select(\"month_offset\").distinct()\n",
        "categories = user_profiles.select(\"category\").distinct()\n",
        "\n",
        "# Create a cross join of users, months, and categories\n",
        "user_months = users.crossJoin(months)\n",
        "all_combinations = user_months.crossJoin(categories)\n",
        "\n",
        "# Join with user profiles, filling in 0% for missing categories\n",
        "full_profiles = all_combinations.join(user_profiles, [\"user1\", \"month_offset\", \"category\"], \"left_outer\") \\\n",
        "                               .withColumn(\"percentage\", when(spark_col(\"percentage\").isNull(), lit(0.0)).otherwise(spark_col(\"percentage\"))) \\\n",
        "                               .select(\"user1\", \"month_offset\", \"category\", \"percentage\")\n",
        "\n",
        "# Compute average and standard deviation across all users for each month and category\n",
        "stats = full_profiles.groupBy(\"month_offset\", \"category\") \\\n",
        "                    .agg(\n",
        "                        spark_sum(lit(1)).alias(\"user_count\"),\n",
        "                        spark_sum(\"percentage\").alias(\"sum_percentage\"),\n",
        "                        spark_sum(spark_col(\"percentage\") * spark_col(\"percentage\")).alias(\"sum_squares\")\n",
        "                    ) \\\n",
        "                    .withColumn(\"mean_percentage\", spark_col(\"sum_percentage\") / spark_col(\"user_count\")) \\\n",
        "                    .withColumn(\"variance\", (spark_col(\"sum_squares\") / spark_col(\"user_count\") - (spark_col(\"sum_percentage\") / spark_col(\"user_count\"))**2)) \\\n",
        "                    .withColumn(\"stddev\", (spark_col(\"variance\")**0.5).cast(\"float\")) \\\n",
        "                    .select(\"month_offset\", \"category\", \"mean_percentage\", \"stddev\")\n",
        "\n",
        "# Collect results for plotting\n",
        "stats_pd = stats.toPandas()\n",
        "\n",
        "# Plot: Average percentage with confidence intervals (Â±2 * stddev)\n",
        "plt.figure(figsize=(12, 8))\n",
        "categories = stats_pd['category'].unique()\n",
        "for category in categories:\n",
        "    cat_data = stats_pd[stats_pd['category'] == category]\n",
        "    months = cat_data['month_offset']\n",
        "    means = cat_data['mean_percentage']\n",
        "    stddevs = cat_data['stddev']\n",
        "    # Only plot categories with significant means\n",
        "    if means.max() > 1:  # Arbitrary threshold to reduce clutter\n",
        "        plt.errorbar(months, means, yerr=2 * stddevs, label=category, capsize=5, marker='o')\n",
        "plt.xlabel(\"Time (Months)\")\n",
        "plt.ylabel(\"Average Percentage\")\n",
        "plt.title(\"Average Spending Profile Over Time with Confidence Intervals (Â±2 SD)\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"/content/average_spending_profile.png\")\n",
        "plt.close()\n",
        "\n",
        "# Print sample statistics\n",
        "print(\"Sample Statistics (first 10 rows):\")\n",
        "stats.show(10, truncate=False)\n",
        "\n",
        "# Save profiles for future use\n",
        "user_profiles.write.mode(\"overwrite\").parquet(\"/content/dynamic_spending_profiles.parquet\")\n",
        "\n",
        "# Stop SparkSession\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDMvLGo_ZYOW"
      },
      "source": [
        "### 1.4 Recent foundation models can capture rich semantic meanings without hand-curated dictionaries. In this task you will use a pre-trained LLaMA 3-8B model (e.g., the Hugging Face checkpoint meta-llama/Meta-Llama-3-8B-Instruct) in Google Colab to embed each Venmo message and discover topics automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2WPFhxfZhAW",
        "outputId": "bfae6b0e-81ac-4eec-d133-d1a83a8a21a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hugging Face token set directly in code.\n",
            "Venmo dataset loaded successfully.\n",
            "User profiles loaded successfully.\n",
            "After filtering empty messages, 36876 messages remain for embedding.\n",
            "Generated embeddings with shape: (36876, 768)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3 clusters (excluding noise).\n",
            "\n",
            "Cluster 0: Cluster_0\n",
            "Top Keywords: uber, uberz, ubers, ubered, uberrr, uber.\n",
            "Top Messages: uber, uber, uber, uber ğŸš–ğŸš–, uber\n",
            "\n",
            "Cluster 1: Food/Drink\n",
            "Top Keywords: food, groceries, foods, snacks, sandwich, for, eyelashes, beverages, food., pancakes\n",
            "Top Messages: food, food, food, food, food\n",
            "\n",
            "Cluster 2: Bills & Utilities\n",
            "Top Keywords: rent, and, rent., rents\n",
            "Top Messages: rent, rent, rent, rent, ğŸ ğŸ’¸ rent\n",
            "Adjusted Rand Index between clustered topics and hand-assigned categories: -0.0047\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install -q transformers accelerate\n",
        "!pip install -q huggingface_hub\n",
        "!pip install -q hdbscan umap-learn scikit-learn\n",
        "\n",
        "# Set the Hugging Face token directly (not recommended for security, but used per user preference)\n",
        "# Note: Token regeneration is recommended, but this is the current token for now\n",
        "import os\n",
        "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"Put_your_tokens\"\n",
        "print(\"Hugging Face token set directly in code.\")\n",
        "\n",
        "# Import libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col as spark_col, udf, row_number\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import StringType, ArrayType, FloatType\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import emoji\n",
        "import re\n",
        "import unicodedata\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "import hdbscan\n",
        "import umap\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"VenmoAnalysisQ4\") \\\n",
        "    .config(\"spark.driver.memory\", \"8g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# File paths\n",
        "venmo_path = \"/content/venmo_cleaned.parquet\"\n",
        "user_profiles_path = \"/content/user_spending_profiles.parquet\"\n",
        "\n",
        "# Load cleaned Venmo dataset\n",
        "try:\n",
        "    df = spark.read.parquet(venmo_path)\n",
        "    print(\"Venmo dataset loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading Venmo dataset: {e}\")\n",
        "    spark.stop()\n",
        "    raise e\n",
        "\n",
        "# Load user profiles from Q2 to get hand-assigned categories\n",
        "try:\n",
        "    user_profiles = spark.read.parquet(user_profiles_path)\n",
        "    print(\"User profiles loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading user profiles: {e}\")\n",
        "    spark.stop()\n",
        "    raise e\n",
        "\n",
        "# Sample 50k messages to manage memory constraints on CPU\n",
        "df_sample = df.select(\"story_id\", \"user1\", \"description\").distinct().sample(fraction=0.01, seed=42).limit(50000)\n",
        "\n",
        "# Remove emojis from messages and filter out empty messages\n",
        "def remove_emojis(text):\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    # Remove emojis using the emoji library\n",
        "    text = emoji.replace_emoji(text, replace=\"\")\n",
        "    # Remove remaining Unicode emojis and symbols using regex\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)  # Remove non-ASCII characters\n",
        "    text = text.strip()\n",
        "    return text if text else None  # Return None if the result is empty\n",
        "\n",
        "remove_emojis_udf = udf(remove_emojis, StringType())\n",
        "df_sample = df_sample.withColumn(\"text_no_emojis\", remove_emojis_udf(spark_col(\"description\")))\n",
        "# Filter out messages that are empty after emoji removal\n",
        "df_sample = df_sample.filter(spark_col(\"text_no_emojis\").isNotNull())\n",
        "\n",
        "messages = df_sample.collect()\n",
        "message_ids = [row[\"story_id\"] for row in messages]\n",
        "user_ids = [row[\"user1\"] for row in messages]\n",
        "message_texts = [row[\"description\"] for row in messages]\n",
        "message_texts_no_emojis = [row[\"text_no_emojis\"] for row in messages]\n",
        "print(f\"After filtering empty messages, {len(messages)} messages remain for embedding.\")\n",
        "\n",
        "# Load DistilBERT model and tokenizer (alternative due to LLaMA 3-8B access issues)\n",
        "model_id = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "model = AutoModel.from_pretrained(model_id, torch_dtype=torch.float16)  # Run on CPU\n",
        "model.eval()\n",
        "\n",
        "# Function to generate embeddings in batches (CPU version)\n",
        "def get_embeddings(texts, batch_size=32):\n",
        "    embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i + batch_size]\n",
        "        # Skip empty batches\n",
        "        batch_texts = [text if text else \" \" for text in batch_texts]\n",
        "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)  # Run on CPU\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            # Use mean of hidden states as embedding\n",
        "            batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "        embeddings.append(batch_embeddings)\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "# Generate embeddings\n",
        "embeddings = get_embeddings(message_texts_no_emojis)\n",
        "print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
        "\n",
        "# Cluster embeddings using HDBSCAN with adjusted parameters\n",
        "clusterer = hdbscan.HDBSCAN(min_cluster_size=200, metric='euclidean')  # Increased min_cluster_size to reduce small clusters\n",
        "cluster_labels = clusterer.fit_predict(embeddings)\n",
        "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)  # Exclude noise (-1)\n",
        "print(f\"Found {n_clusters} clusters (excluding noise).\")\n",
        "\n",
        "# Label clusters: Get top 10 representative messages per cluster\n",
        "df_clusters = pd.DataFrame({\n",
        "    \"story_id\": message_ids,\n",
        "    \"user1\": user_ids,\n",
        "    \"message\": message_texts,\n",
        "    \"text_no_emojis\": message_texts_no_emojis,\n",
        "    \"cluster\": cluster_labels\n",
        "})\n",
        "\n",
        "# Function to extract keywords (simple word frequency for now)\n",
        "def get_top_keywords(texts, n=10):\n",
        "    words = \" \".join(texts).lower().split()\n",
        "    words = [word for word in words if len(word) > 2]  # Ignore short words\n",
        "    word_counts = pd.Series(words).value_counts()\n",
        "    return word_counts.head(n).index.tolist()\n",
        "\n",
        "# Analyze each cluster\n",
        "cluster_summary = {}\n",
        "for cluster_id in set(cluster_labels):\n",
        "    if cluster_id == -1:  # Skip noise\n",
        "        continue\n",
        "    cluster_data = df_clusters[df_clusters[\"cluster\"] == cluster_id]\n",
        "    top_messages = cluster_data[\"message\"].head(10).tolist()\n",
        "    top_keywords = get_top_keywords(cluster_data[\"text_no_emojis\"])\n",
        "    # Assign a short name based on keywords\n",
        "    cluster_name = \"Cluster_\" + str(cluster_id)  # Placeholder\n",
        "    if \"food\" in top_keywords or \"dinner\" in top_keywords or \"lunch\" in top_keywords:\n",
        "        cluster_name = \"Food/Drink\"\n",
        "    elif \"rent\" in top_keywords or \"bill\" in top_keywords or \"electricity\" in top_keywords:\n",
        "        cluster_name = \"Bills & Utilities\"\n",
        "    elif \"friend\" in top_keywords or \"thanks\" in top_keywords or \"bff\" in top_keywords:\n",
        "        cluster_name = \"Social Payments\"\n",
        "    elif \"trip\" in top_keywords or \"travel\" in top_keywords or \"flight\" in top_keywords:\n",
        "        cluster_name = \"Travel\"\n",
        "    elif \"cash\" in top_keywords or \"pay\" in top_keywords or \"money\" in top_keywords:\n",
        "        cluster_name = \"Cash Transactions\"\n",
        "    cluster_summary[cluster_id] = {\n",
        "        \"name\": cluster_name,\n",
        "        \"top_messages\": top_messages,\n",
        "        \"top_keywords\": top_keywords\n",
        "    }\n",
        "\n",
        "# Print cluster summaries\n",
        "for cluster_id, summary in cluster_summary.items():\n",
        "    print(f\"\\nCluster {cluster_id}: {summary['name']}\")\n",
        "    print(\"Top Keywords:\", \", \".join(summary['top_keywords']))\n",
        "    print(\"Top Messages:\", \", \".join(summary['top_messages'][:5]))  # Show first 5 for brevity\n",
        "\n",
        "# Map messages to hand-assigned categories\n",
        "# Use a window function to find the category with the maximum percentage for each user\n",
        "window_spec = Window.partitionBy(\"user1\").orderBy(spark_col(\"percentage\").desc())\n",
        "user_profiles_ranked = user_profiles.withColumn(\"rank\", row_number().over(window_spec))\n",
        "user_dominant_category = user_profiles_ranked.filter(spark_col(\"rank\") == 1).select(\"user1\", spark_col(\"category\").alias(\"dominant_category\"))\n",
        "\n",
        "# Join with df_clusters to assign dominant categories\n",
        "df_sample_with_user = df_clusters.merge(user_dominant_category.toPandas(), on=\"user1\", how=\"left\")\n",
        "df_sample_with_user = df_sample_with_user.dropna(subset=[\"dominant_category\"])  # Drop rows with missing categories\n",
        "\n",
        "# Compute Adjusted Rand Index\n",
        "true_labels = df_sample_with_user[\"dominant_category\"]\n",
        "pred_labels = df_sample_with_user[\"cluster\"]\n",
        "ari_score = adjusted_rand_score(true_labels, pred_labels)\n",
        "print(f\"Adjusted Rand Index between clustered topics and hand-assigned categories: {ari_score:.4f}\")\n",
        "\n",
        "# Reduce embeddings to 2D with UMAP and plot\n",
        "reducer = umap.UMAP(n_components=2, random_state=42)\n",
        "embeddings_2d = reducer.fit_transform(embeddings)\n",
        "df_clusters[\"x\"] = embeddings_2d[:, 0]\n",
        "df_clusters[\"y\"] = embeddings_2d[:, 1]\n",
        "df_clusters[\"cluster_label\"] = df_clusters[\"cluster\"].apply(\n",
        "    lambda x: cluster_summary[x][\"name\"] if x != -1 else \"Noise\"\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(data=df_clusters, x=\"x\", y=\"y\", hue=\"cluster_label\", palette=\"deep\", alpha=0.6)\n",
        "plt.title(\"Venmo Message Clusters (UMAP Projection)\")\n",
        "plt.xlabel(\"UMAP Dimension 1\")\n",
        "plt.ylabel(\"UMAP Dimension 2\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"/content/message_clusters_umap.png\")\n",
        "plt.close()\n",
        "\n",
        "# Stop SparkSession\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdjqDeQUjmIz"
      },
      "source": [
        "## 2. Social Network Analytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLx1p8qxjw4A"
      },
      "source": [
        "### Q5: Write a script to identify each userâ€™s friends and friends of friends. (Definition: A friend is anyone who has transacted with the userâ€”either by sending or receiving money.)\n",
        "*   Describe your algorithm clearly.\n",
        "*   Calculate its computational complexity.\n",
        "*   Can you improve its efficiency?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kft5g_mSjtu9"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import (col, collect_set, explode, array_except, array,)\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"VenmoAnalysisQ5\") \\\n",
        "    .config(\"spark.driver.memory\", \"8g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# File paths\n",
        "venmo_path = \"/content/VenmoSample.snappy.parquet\"\n",
        "\n",
        "# Load cleaned Venmo dataset\n",
        "try:\n",
        "    df = spark.read.parquet(venmo_path)\n",
        "    print(\"Venmo dataset loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading Venmo dataset: {e}\")\n",
        "    spark.stop()\n",
        "    raise e\n",
        "\n",
        "# Make the relation between users undirected, flip each edge by adding its reverse\n",
        "allusers = (\n",
        "    df.select(col(\"user1\").alias(\"u\"), col(\"user2\").alias(\"v\"))\n",
        "      .union(df.select(col(\"user2\").alias(\"u\"), col(\"user1\").alias(\"v\")))\n",
        "      .distinct())\n",
        "\n",
        "# Compute direct friends, group by u first and find all the v it has\n",
        "friends = allusers.groupBy(\"u\") \\\n",
        "               .agg(collect_set(\"v\").alias(\"friends\"))\n",
        "\n",
        "# Turns each userâ€™s friend list into one row per user per friend\n",
        "friend_list = friends.select(\n",
        "    col(\"u\").alias(\"user\"),\n",
        "    explode(\"friends\").alias(\"friend\"))\n",
        "\n",
        "# For each of user's friends, looks up their friends, aggregates back into the listed created above\n",
        "friend_of_friends = (\n",
        "    friend_list\n",
        "    .join(friends.withColumnRenamed(\"u\", \"friend\"), on=\"friend\")\n",
        "    .select(\n",
        "        col(\"user\"),\n",
        "        explode(\"friends\").alias(\"fof_candidate\"))\n",
        "    .groupBy(\"user\")\n",
        "    .agg(collect_set(\"fof_candidate\").alias(\"fof_all\")))\n",
        "\n",
        "# Clean the friend_of_friends by removing any repeating direct friends and user themself\n",
        "result = (\n",
        "    friends\n",
        "    .select(col(\"u\").alias(\"user\"), \"friends\")\n",
        "    .join(friend_of_friends, on=\"user\", how=\"left\")\n",
        "    .withColumn(\n",
        "        \"friends_of_friends\",\n",
        "        # first remove direct friends, then remove the user\n",
        "        array_except(\n",
        "          array_except(col(\"fof_all\"), col(\"friends\")),\n",
        "          array(col(\"user\"))))\n",
        "    .select(\"user\", \"friends\", \"friends_of_friends\"))\n",
        "\n",
        "# Show the results\n",
        "result.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQd83Epvjxvj"
      },
      "outputs": [],
      "source": [
        "# Calculate its computational complexity\n",
        "from pyspark.sql.functions import countDistinct, size, avg\n",
        "\n",
        "# Calculate the number of users\n",
        "num_users = result.select(\"user\").distinct().count()\n",
        "\n",
        "# Average number of direct friends per user\n",
        "avg_friends = result.select(size(\"friends\").alias(\"num_friends\")) \\\n",
        "                    .agg(avg(\"num_friends\")).first()[0]\n",
        "\n",
        "# Total number of friend-of-friend pairs\n",
        "fof_pairs = result.select(\"user\", explode(\"friends_of_friends\").alias(\"fof\"))\n",
        "num_fof_pairs = fof_pairs.count()\n",
        "\n",
        "print(f\"Number of users (n): {num_users}\")\n",
        "print(f\"Average number of friends per user (d): {avg_friends:.2f}\")\n",
        "print(f\"Estimated n Ã— dÂ²: {int(num_users * (avg_friends ** 2))}\")\n",
        "print(f\"Actual number of user and their friend of friend pairs: {num_fof_pairs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7UyaHkdjwnr"
      },
      "outputs": [],
      "source": [
        "# Stop SparkSession\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxVgQJG9jyLX"
      },
      "source": [
        "### Q6: Now that you have each userâ€™s list of friends and friends of friends, you're ready to compute various social network metrics. Using the same dynamic framework from earlier, calculate the following metrics over the userâ€™s lifetime on Venmo, from month 0 through month 12.\n",
        "*   i) Number of friends and number of friends of friends."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CUMOYThj3Ju"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import (col, least, floor, months_between,min as spark_min,\n",
        "                                   explode, collect_set, size, lit, coalesce, array,\n",
        "                                   posexplode, when, coalesce, expr, desc)\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Initialize the Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"VenmoAnalysisQ6\") \\\n",
        "    .config(\"spark.driver.memory\",\"8g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# File paths\n",
        "venmo_path = \"/content/VenmoSample.snappy.parquet\"\n",
        "\n",
        "# Load cleaned Venmo dataset, sample with 1% of the data\n",
        "try:\n",
        "    df = spark.read.parquet(venmo_path).sample(fraction=0.01, seed=42)\n",
        "    print(\"Venmo dataset loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading Venmo dataset: {e}\")\n",
        "    spark.stop()\n",
        "    raise e\n",
        "\n",
        "# Join all the users together\n",
        "users1 = df.select(col(\"user1\").alias(\"user\"), \"datetime\")\n",
        "users2 = df.select(col(\"user2\").alias(\"user\"), \"datetime\")\n",
        "all_users = users1.union(users2)\n",
        "\n",
        "# Compute each userâ€™s firstâ€ever transaction datetime\n",
        "first_month = (all_users\n",
        "              .withColumn(\"first_month\", spark_min(\"datetime\").over(Window.partitionBy(\"user\")))\n",
        "              .select(\"user\",\"first_month\")\n",
        "              .distinct())\n",
        "\n",
        "# Build the undirected between users and friends with datetime, with months limit to 12\n",
        "edge1 = (df.join(first_month, df.user1 == first_month.user)\n",
        "        # Keep 12 months since the first transcation\n",
        "        .withColumn(\"month\", least(floor(months_between(col(\"datetime\"), col(\"first_month\"))),lit(12)))\n",
        "        .select(col(\"user1\").alias(\"user\"), col(\"user2\").alias(\"friend\"), \"month\"))\n",
        "\n",
        "edge1 = (df.join(first_month, df.user2 == first_month.user)\n",
        "        .withColumn(\"month\", least(floor(months_between(col(\"datetime\"), col(\"first_month\"))),lit(12)))\n",
        "        .select(col(\"user2\").alias(\"user\"), col(\"user1\").alias(\"friend\"), \"month\"))\n",
        "\n",
        "edges = edge1.union(edge1).distinct()\n",
        "\n",
        "# Creat a loop to compute the cumulative friends up to 12 months\n",
        "results = []\n",
        "for m in range(0,13):\n",
        "    # Filter for cumulative graph up to month m\n",
        "    sub = edges.filter(col(\"month\") <= lit(m))\n",
        "\n",
        "    # Calculate direct friends by month m\n",
        "    direct_friend = (sub\n",
        "                 .groupBy(\"user\")\n",
        "                 .agg(collect_set(\"friend\").alias(\"friends\")))\n",
        "\n",
        "    # Calculate friendsâ€ofâ€friends by month m\n",
        "    fof = (direct_friend\n",
        "            .select(col(\"user\"), explode(\"friends\").alias(\"f\"))\n",
        "            .join(direct_friend.select(col(\"user\").alias(\"f\"), col(\"friends\").alias(\"f_friends\")),on=\"f\", how=\"left\")\n",
        "            .select(col(\"user\"), explode(\"f_friends\").alias(\"fof\"))\n",
        "            .filter(col(\"fof\") != col(\"user\"))\n",
        "            .groupBy(\"user\")\n",
        "            .agg(collect_set(\"fof\").alias(\"fof_set\")))\n",
        "\n",
        "    # Join all the results\n",
        "    df_month = (direct_friend.join(fof, on=\"user\", how=\"left\")\n",
        "              .select(col(\"user\"),\n",
        "                      lit(m).alias(\"month\"),\n",
        "                      size(\"friends\").alias(\"num_friends\"),\n",
        "                      size(coalesce(col(\"fof_set\"), array().cast(\"array<long>\"))).alias(\"num_fof\")))\n",
        "\n",
        "    results.append(df_month)\n",
        "\n",
        "# Union all months\n",
        "dynamic = results[0]\n",
        "for part in results[1:]:\n",
        "    dynamic = dynamic.union(part)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wUc4hInj2__"
      },
      "outputs": [],
      "source": [
        "# Show the result\n",
        "dynamic.show(20, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh9GJBMDj-Nm"
      },
      "source": [
        "###    ii) Clustering coefficient of each userâ€™s network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYaKRzF9kB1V"
      },
      "outputs": [],
      "source": [
        "# Change the loop in previous questions to calculate the clustering coefficient\n",
        "results = []\n",
        "\n",
        "for m in range(0, 13):\n",
        "    # Filter for cumulative graph up to month m\n",
        "    sub = edges.filter(col(\"month\") <= lit(m))\n",
        "\n",
        "    # Direct friends\n",
        "    direct_friend = sub.groupBy(\"user\") \\\n",
        "                .agg(collect_set(\"friend\").alias(\"friends\"))\n",
        "\n",
        "    # Friendsâ€ofâ€friends\n",
        "    fof = (direct_friend\n",
        "          .select(col(\"user\"), explode(\"friends\").alias(\"f\"))\n",
        "          .join(direct_friend.select(col(\"user\").alias(\"f\"), col(\"friends\").alias(\"f_friends\")),on=\"f\", how=\"left\")\n",
        "          .select(col(\"user\"), explode(\"f_friends\").alias(\"fof\"))\n",
        "          .filter(col(\"fof\") != col(\"user\"))\n",
        "          .groupBy(\"user\")\n",
        "          .agg(collect_set(\"fof\").alias(\"fof_set\")))\n",
        "\n",
        "    # Build  unique friendâ€pairs for each user\n",
        "    friend_i = direct_friend.select(\"user\", posexplode(\"friends\").alias(\"i\",\"friend1\"))\n",
        "    friend_j = direct_friend.select(\"user\", posexplode(\"friends\").alias(\"j\",\"friend2\"))\n",
        "\n",
        "    pairs = (\n",
        "      friend_i\n",
        "      .join(friend_j, on=\"user\")\n",
        "      .filter(col(\"i\") < col(\"j\")) # Keep only rows with i < j, to have unique friend pairs\n",
        "      .select(\"user\",\"friend1\",\"friend2\"))\n",
        "\n",
        "    # Count actual links among friends\n",
        "    links = (pairs\n",
        "      .join(\n",
        "        sub.select(col(\"user\").alias(\"user1\"), col(\"friend\").alias(\"user2\")).distinct(),\n",
        "        (col(\"friend1\")==col(\"user1\")) & (col(\"friend2\")==col(\"user2\")),how=\"inner\")\n",
        "      .groupBy(\"user\")\n",
        "      .count()\n",
        "      .withColumnRenamed(\"count\",\"Links\"))\n",
        "\n",
        "    # Bring it all together into dfm\n",
        "    df_month = (direct_friend\n",
        "      .join(fof, on=\"user\", how=\"left\")\n",
        "      .join(links, on=\"user\", how=\"left\")\n",
        "      .select(\n",
        "         col(\"user\"),\n",
        "         lit(m).alias(\"month\"),\n",
        "         size(\"friends\").alias(\"num_friends\"),\n",
        "         size(\"fof_set\").alias(\"num_fof\"),\n",
        "         # replace NULL L with 0\n",
        "         coalesce(col(\"Links\"), lit(0)).alias(\"Links\")\n",
        "      )\n",
        "      .withColumn(\n",
        "        \"clustering_coeff\",\n",
        "        # Clustering Coefficient: Links / (k*(k-1)/2), but guard divideâ€byâ€zero\n",
        "        when(col(\"num_friends\") >= 2,\n",
        "             col(\"Links\") / (col(\"num_friends\")*(col(\"num_friends\")-1)/2)\n",
        "        ).otherwise(lit(0.0))\n",
        "      ))\n",
        "\n",
        "    results.append(df_month)\n",
        "\n",
        "# Union all months into one DataFrame\n",
        "dynamic_net = results[0]\n",
        "for r in results[1:]:\n",
        "    dynamic_net = dynamic_net.union(r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTacoYGColS-"
      },
      "outputs": [],
      "source": [
        "# Show the result\n",
        "dynamic_net.orderBy(desc(\"month\")).show(20, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMoAA-46onIW"
      },
      "source": [
        "### iii) PageRank of each user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svZS943MorEx"
      },
      "outputs": [],
      "source": [
        "# As GraphFrames is not supported with Spark 3.5, we use Networkx for this problem\n",
        "import networkx as nx\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import col, coalesce, lit, desc\n",
        "\n",
        "# Collect as edge list\n",
        "edges = df.select(\"user1\", \"user2\").rdd.map(lambda row: (row.user1, row.user2)).collect()\n",
        "\n",
        "# Build undirected graph using networkx\n",
        "graph = nx.Graph()\n",
        "graph.add_edges_from(edges)\n",
        "\n",
        "# Compute PageRank\n",
        "pr_dict = nx.pagerank(graph, alpha=0.85, max_iter=100)\n",
        "\n",
        "# Convert to Spark DataFrame\n",
        "pr_rows = [Row(user=int(k), pagerank=float(v)) for k, v in pr_dict.items()]\n",
        "pr_df = spark.createDataFrame(pr_rows)\n",
        "\n",
        "# Join PageRank into each user's per-month profile\n",
        "dynamic_pagerank = dynamic_net.join(pr_df, on=\"user\", how=\"left\")\n",
        "\n",
        "# Replace null or unsupported values in num_fof with 0\n",
        "dynamic_pagerank = dynamic_pagerank.withColumn(\"num_fof\",\n",
        "    coalesce(col(\"num_fof\"), lit(0)))\n",
        "\n",
        "# Show result\n",
        "dynamic_pagerank.orderBy(desc(\"month\")).show(20, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hc1lbf4ovlNQ"
      },
      "source": [
        "## Predictive Analytics with MLlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS3t5ZEAvvTj"
      },
      "source": [
        "### Q7: Create the dependent variable Y, defined as the total number of transactions a user makes during their first 12 months on Venmo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Kjz2K3IvtNS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
